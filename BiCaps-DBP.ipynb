{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltosEsKlm4CJ"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import json\n",
    "from keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    InputLayer,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Embedding,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Convolution1D,\n",
    "    MaxPooling1D,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH5JvQszm5Q5"
   },
   "outputs": [],
   "source": [
    "def create_dataset(data_path: str) -> Tuple[List[str], List[int]]:\n",
    "    dataset = pd.read_csv(data_path)\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)  # shuffle the dataset\n",
    "    return list(dataset[\"sequence\"]), list(dataset[\"label\"])\n",
    "\n",
    "\n",
    "def split_dataset(\n",
    "    sequences_list: List[str], labels_list: List[int], train_size: float = 0.8\n",
    ") -> Tuple[List[str], List[str], List[str], List[int], List[int], List[int]]:\n",
    "    dataset = pd.DataFrame({\"sequence\": sequences_list, \"label\": labels_list})\n",
    "    dataset = dataset.sample(frac=1, random_state=1)\n",
    "    train, remaining = train_test_split(dataset, train_size=train_size, random_state=2)\n",
    "    valid, test = train_test_split(remaining, test_size=0.5, random_state=3)\n",
    "    x_train, x_valid, x_test = train[\"sequence\"], valid[\"sequence\"], test[\"sequence\"]\n",
    "    y_train, y_valid, y_test = train[\"label\"], valid[\"label\"], test[\"label\"]\n",
    "    return (\n",
    "        list(x_train),\n",
    "        list(x_valid),\n",
    "        list(x_test),\n",
    "        list(y_train),\n",
    "        list(y_valid),\n",
    "        list(y_test),\n",
    "    )\n",
    "\n",
    "def one_hot_encoding(\n",
    "    sequence: str,\n",
    "    max_seq_length: int = 800,\n",
    "    CONSIDERED_AA: str = \"ACDEFGHIKLMNPQRSTVWY\",\n",
    "):\n",
    "    # adapt sequence size\n",
    "    if len(sequence) > max_seq_length:\n",
    "        # short the sequence\n",
    "        sequence = sequence[:max_seq_length]\n",
    "    else:\n",
    "        # pad the sequence\n",
    "        sequence = sequence + \".\" * (max_seq_length - len(sequence))\n",
    "\n",
    "    # encode sequence\n",
    "    encoded_sequence = np.zeros((max_seq_length, len(CONSIDERED_AA)))  # (1000, 20)\n",
    "    for i, amino_acid in enumerate(sequence):\n",
    "        if amino_acid in CONSIDERED_AA:\n",
    "            encoded_sequence[i][CONSIDERED_AA.index(amino_acid)] = 1\n",
    "    model_input = np.expand_dims(encoded_sequence, 0)  # add batch dimension\n",
    "\n",
    "    return model_input  # (1, 1000, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwXTGbfvm5Ni"
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bicaps-DBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgFPcZTnownx"
   },
   "outputs": [],
   "source": [
    "from keras.backend import *\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "\n",
    "#own_batch_dot = batch_dot  # force standard implementation \n",
    "\n",
    "# import of batch_dot operation from TF 1.13\n",
    "# https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/keras/backend.py\n",
    "\n",
    "def own_batch_dot(x, y, axes=None):\n",
    "  \"\"\"Batchwise dot product.\n",
    "  `batch_dot` is used to compute dot product of `x` and `y` when\n",
    "  `x` and `y` are data in batch, i.e. in a shape of\n",
    "  `(batch_size, :)`.\n",
    "  `batch_dot` results in a tensor or variable with less dimensions\n",
    "  than the input. If the number of dimensions is reduced to 1,\n",
    "  we use `expand_dims` to make sure that ndim is at least 2.\n",
    "  Arguments:\n",
    "      x: Keras tensor or variable with `ndim >= 2`.\n",
    "      y: Keras tensor or variable with `ndim >= 2`.\n",
    "      axes: list of (or single) int with target dimensions.\n",
    "          The lengths of `axes[0]` and `axes[1]` should be the same.\n",
    "  Returns:\n",
    "      A tensor with shape equal to the concatenation of `x`'s shape\n",
    "      (less the dimension that was summed over) and `y`'s shape\n",
    "      (less the batch dimension and the dimension that was summed over).\n",
    "      If the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
    "  Examples:\n",
    "      Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
    "      `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
    "      of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
    "      elements.\n",
    "      Shape inference:\n",
    "      Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
    "      If `axes` is (1, 2), to find the output shape of resultant tensor,\n",
    "          loop through each dimension in `x`'s shape and `y`'s shape:\n",
    "      * `x.shape[0]` : 100 : append to output shape\n",
    "      * `x.shape[1]` : 20 : do not append to output shape,\n",
    "          dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
    "      * `y.shape[0]` : 100 : do not append to output shape,\n",
    "          always ignore first dimension of `y`\n",
    "      * `y.shape[1]` : 30 : append to output shape\n",
    "      * `y.shape[2]` : 20 : do not append to output shape,\n",
    "          dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
    "      `output_shape` = `(100, 30)`\n",
    "  ```python\n",
    "      >>> x_batch = K.ones(shape=(32, 20, 1))\n",
    "      >>> y_batch = K.ones(shape=(32, 30, 20))\n",
    "      >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
    "      >>> K.int_shape(xy_batch_dot)\n",
    "      (32, 1, 30)\n",
    "  ```\n",
    "  \"\"\"\n",
    "  if isinstance(axes, int):\n",
    "    axes = (axes, axes)\n",
    "  x_ndim = ndim(x)\n",
    "  y_ndim = ndim(y)\n",
    "  if axes is None:\n",
    "    # behaves like tf.batch_matmul as default\n",
    "    axes = [x_ndim - 1, y_ndim - 2]\n",
    "  if x_ndim > y_ndim:\n",
    "    diff = x_ndim - y_ndim\n",
    "    y = array_ops.reshape(y,\n",
    "                          array_ops.concat(\n",
    "                              [array_ops.shape(y), [1] * (diff)], axis=0))\n",
    "  elif y_ndim > x_ndim:\n",
    "    diff = y_ndim - x_ndim\n",
    "    x = array_ops.reshape(x,\n",
    "                          array_ops.concat(\n",
    "                              [array_ops.shape(x), [1] * (diff)], axis=0))\n",
    "  else:\n",
    "    diff = 0\n",
    "  if ndim(x) == 2 and ndim(y) == 2:\n",
    "    if axes[0] == axes[1]:\n",
    "      out = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
    "    else:\n",
    "      out = math_ops.reduce_sum(\n",
    "          math_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
    "  else:\n",
    "    adj_x = None if axes[0] == ndim(x) - 1 else True\n",
    "    adj_y = True if axes[1] == ndim(y) - 1 else None\n",
    "    out = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
    "  if diff:\n",
    "    if x_ndim > y_ndim:\n",
    "      idx = x_ndim + y_ndim - 3\n",
    "    else:\n",
    "      idx = x_ndim - 1\n",
    "    out = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
    "  if ndim(out) == 1:\n",
    "    out = expand_dims(out, 1)\n",
    "  return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1pzP81km5J5"
   },
   "outputs": [],
   "source": [
    "from keras import initializers, layers\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=tf.shape(x)[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routing: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=(self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: own_batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        b = K.zeros(shape=[self.batch_size, self.num_capsule, self.input_num_capsule])\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=1)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))\n",
    "            if i != 1:\n",
    "                b = b + K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), b, K.sum(inputs_hat, 2, keepdims=False)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.input_num_capsule]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.dim_capsule])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # In forward pass, `inputs_hat_stopped` = `inputs_hat`;\n",
    "        # In backward, no gradient can flow from `inputs_hat_stopped` back to `inputs_hat`.\n",
    "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
    "        \n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=(K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule))\n",
    "\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, axis=1)\n",
    "\n",
    "            # At last iteration, use `inputs_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.num_routing - 1:\n",
    "                # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "                # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "                outputs = squash(own_batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            else:  # Otherwise, use `inputs_hat_stopped` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(own_batch_dot(c, inputs_hat_stopped, [2, 2]))\n",
    "\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += own_batch_dot(outputs, inputs_hat_stopped, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv1D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oLA8_rzm5HC"
   },
   "outputs": [],
   "source": [
    "def CapsNet(input_shape,top_words, maxlen, n_class, routings):\n",
    "    \"\"\"\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "    # Bi-LSTM layer\n",
    "    s= Bidirectional(LSTM(256,return_sequences=True))(x)\n",
    "   \n",
    "    # Layer 1: Just a conventional Conv1D layer\n",
    "    conv1 = layers.Conv1D(filters=256, kernel_size=12, strides=1, padding='valid', activation='relu', name='conv1')(s)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=5, n_channels=10, kernel_size=13, strides=2, padding='valid')\n",
    "    \n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=5, num_routing=routings,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "    \n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    out_caps = Length(name='capsnet')(digitcaps) \n",
    "\n",
    "     # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model(x,out_caps) #masked_by_y\n",
    "    \n",
    "    return train_model\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdTTDoqn8Xwn"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    SEED = 42\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    TRAIN_SET = \"DNA Binding Protein/Dbp app/data/PDB14189.csv\"\n",
    "    TEST_SET = \"DNA Binding Protein/Dbp app/data/PDB2272.csv\"\n",
    "    \n",
    "\n",
    "    # embedding and convolution parameters\n",
    "    MAX_SEQ_LENGTH = 1000\n",
    "    CONSIDERED_AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    VOCAB_SIZE = len(CONSIDERED_AA)\n",
    "  \n",
    "   # create train dataset\n",
    "    sequences_train, labels_train = create_dataset(data_path=TRAIN_SET)\n",
    "\n",
    "    # create test dataset\n",
    "    sequences_test, labels_test = create_dataset(data_path=TEST_SET)\n",
    "\n",
    "    # encode sequences\n",
    "    sequences_train_encoded = np.concatenate(\n",
    "        [\n",
    "            one_hot_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
    "            for seq in sequences_train\n",
    "        ],\n",
    "        axis=0,\n",
    "    )  # (14189, 1000, 20)\n",
    "    sequences_test_encoded = np.concatenate(\n",
    "        [\n",
    "            one_hot_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
    "            for seq in sequences_test\n",
    "        ],\n",
    "        axis=0,\n",
    "    )  # (2272, 1000, 20)\n",
    "\n",
    "    # encode labels\n",
    "    labels_train_encoded = to_categorical(\n",
    "        labels_train, num_classes=2, dtype=\"float32\"\n",
    "    )  # (14189, 2)\n",
    "    labels_test_encoded = to_categorical(\n",
    "        labels_test, num_classes=2, dtype=\"float32\"\n",
    "    )  # (2272, 2)\n",
    "       \n",
    "   \n",
    "    return sequences_train_encoded, labels_train_encoded, sequences_test_encoded, labels_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdtDYTAXOAAz"
   },
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DurJzQQ3GTFt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "sp_per_fold = []\n",
    "prec_per_fold = []\n",
    "sn_per_fold = []\n",
    "AUC_per_fold = []\n",
    "MCC_per_fold = []\n",
    "epoch=30\n",
    "\n",
    "# unpacking the data\n",
    "X_train,y_train,X_test,y_test = load_data()\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# embedding and convolution parameters\n",
    "MAX_SEQ_LENGTH = 1000\n",
    "VOCAB_SIZE = len(CONSIDERED_AA)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "  model, eval_model = CapsNet(X_train[1,:].shape, VOCAB_SIZE, MAX_SEQ_LENGTH, n_class=2, routings=2)\n",
    "  model.summary()\n",
    "\n",
    "# compile the model\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=[margin_loss, 'binary_crossentropy'],\n",
    "                  metrics=[\"accuracy\", specificity, \"Precision\", \"Recall\", \"AUC\", matthews_correlation_coefficient])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=128,\n",
    "              epochs=epoch,\n",
    "              verbose=1)\n",
    "    \n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]} of {scores[2]}; {model.metrics_names[3]} of {scores[3]}; {model.metrics_names[4]} of {scores[4]}; {model.metrics_names[5]} of {scores[5]}; {model.metrics_names[6]} of {scores[6]}')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "  sp_per_fold.append(scores[2])\n",
    "  prec_per_fold.append(scores[3])\n",
    "  sn_per_fold.append(scores[4])\n",
    "  AUC_per_fold.append(scores[5])\n",
    "  MCC_per_fold.append(scores[6])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - Specificity: {sp_per_fold[i]} - Precision: {prec_per_fold[i]} - Sensitivity: {sn_per_fold[i]} - AUC: {AUC_per_fold[i]} - MCC: {MCC_per_fold[i]}')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> Specificity: {np.mean(sp_per_fold)}')\n",
    "print(f'> Precision: {np.mean(prec_per_fold)}')\n",
    "print(f'> Sensitivity: {np.mean(sn_per_fold)}')\n",
    "print(f'> AUC: {np.mean(AUC_per_fold)}')\n",
    "print(f'> MCC: {np.mean(MCC_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMt4g0Lkd6yvYEFc6lfSdJi",
   "background_execution": "on",
   "collapsed_sections": [
    "HdtDYTAXOAAz",
    "Sj_omB4qGWgl",
    "iBNzaOHPKO4b"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
